{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CQF Final Project: Blending Ensemble for Classification\n",
    "\n",
    "**Topic**: ML - Blending Ensemble for Classification  \n",
    "**Asset**: SPY ETF  \n",
    "**Prediction**: 5-day forward return direction (binary classification)  \n",
    "**Author**: LIU JingZhong Eric  \n",
    "**Date**: 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Blending Ensemble Model\n",
    "\n",
    "- **Blending**: Split training data into base_train (70%) and blend_holdout (30%)\n",
    "  - Base learners trained on base_train\n",
    "  - Base learners predict on blend_holdout to create meta-features\n",
    "  - Meta-learner trained on blend_holdout meta-features\n",
    "  \n",
    "\n",
    "This implementation follows the **BLENDING** approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, \n",
    "    precision_recall_curve, ConfusionMatrixDisplay, roc_curve, \n",
    "    auc, matthews_corrcoef, balanced_accuracy_score, f1_score,\n",
    "    brier_score_loss, average_precision_score\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.base import clone\n",
    "from scipy.stats import pearsonr, randint as sp_randint, uniform as sp_uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.calibration import calibration_curve\n",
    "import os\n",
    "import matplotlib.ticker as mtick\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Random Seed Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 0. RANDOM SEED CONTROL\n",
    "# ==========================================\n",
    "def set_seeds(seed=42):\n",
    "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. DATA LOADING AND MERGING\n",
    "# ==========================================\n",
    "def load_and_merge(spy_train, spy_test, gdp_file, dgs10_file):\n",
    "    \"\"\"\n",
    "    Load SPY price data and merge with macroeconomic indicators.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STEP 1: Loading Data\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    df_train_raw = pd.read_csv(spy_train, parse_dates=['Date'], index_col='Date').sort_index()\n",
    "    df_test_raw = pd.read_csv(spy_test, parse_dates=['Date'], index_col='Date').sort_index()\n",
    "    \n",
    "    oos_start_date = df_test_raw.index.min()\n",
    "    print(f\"  Training period: {df_train_raw.index.min()} to {df_train_raw.index.max()}\")\n",
    "    print(f\"  Test period: {df_test_raw.index.min()} to {df_test_raw.index.max()}\")\n",
    "    \n",
    "    df = pd.concat([df_train_raw, df_test_raw]).sort_index()\n",
    "    \n",
    "    # Clean price columns\n",
    "    for col in ['Close/Last', 'Open', 'High', 'Low']:\n",
    "        if col in df.columns and df[col].dtype == 'object':\n",
    "            df[col] = df[col].str.replace('$', '').astype(float)\n",
    "    \n",
    "    if 'Close/Last' in df.columns:\n",
    "        df.rename(columns={'Close/Last': 'Close'}, inplace=True)\n",
    "    \n",
    "    # Load macroeconomic data\n",
    "    gdp = pd.read_csv(gdp_file, parse_dates=['observation_date'])\n",
    "    gdp.columns = ['Date', 'GDPC1']\n",
    "    gdp.set_index('Date', inplace=True)\n",
    "    \n",
    "    dgs10 = pd.read_csv(dgs10_file, parse_dates=['observation_date'])\n",
    "    dgs10.columns = ['Date', 'DGS10']\n",
    "    dgs10['DGS10'] = pd.to_numeric(dgs10['DGS10'], errors='coerce')\n",
    "    dgs10.set_index('Date', inplace=True)\n",
    "    \n",
    "    df = df.join(gdp.reindex(df.index, method='ffill'))\n",
    "    df = df.join(dgs10.reindex(df.index, method='ffill'))\n",
    "    df = df.ffill().bfill()\n",
    "    \n",
    "    print(f\"  Total samples: {len(df)}\")\n",
    "    print(f\"  Date range: {df.index.min()} to {df.index.max()}\")\n",
    "    \n",
    "    return df, oos_start_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Fractional Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. FRACTIONAL DIFFERENTIATION\n",
    "# ==========================================\n",
    "def get_weights_ffd(d, thres, lim):\n",
    "    \"\"\"Calculate weights for fractional differentiation\"\"\"\n",
    "    w, k = [1.], 1\n",
    "    while True:\n",
    "        w_k = -w[-1] / k * (d - k + 1)\n",
    "        if abs(w_k) < thres:\n",
    "            break\n",
    "        w.append(w_k)\n",
    "        k += 1\n",
    "        if k >= lim:\n",
    "            break\n",
    "    return np.array(w[::-1]).reshape(-1, 1)\n",
    "\n",
    "def frac_diff_ffd(series, d=0.4, thres=1e-5):\n",
    "    \"\"\"Apply fractional differentiation to make series stationary while preserving memory.\"\"\"\n",
    "    w = get_weights_ffd(d, thres, len(series))\n",
    "    width = len(w) - 1\n",
    "    df = {}\n",
    "    for name in series.columns:\n",
    "        series_f = series[[name]].ffill().dropna()\n",
    "        df_temp = pd.Series(0.0, index=series_f.index)\n",
    "        for k in range(width, len(series_f)):\n",
    "            df0 = series_f.iloc[k-width:k+1].values\n",
    "            df_temp.iloc[k] = np.dot(w.T, df0)[0, 0]\n",
    "        df[name] = df_temp.loc[series_f.index[width]:]\n",
    "    return pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. COMPREHENSIVE FEATURE ENGINEERING\n",
    "# ==========================================\n",
    "def create_features_enhanced(df):\n",
    "    \"\"\"\n",
    "    Create 40+ features across multiple categories.\n",
    "    All features use LAGGED data to prevent look-ahead bias.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STEP 2: Feature Engineering\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Create lagged price/volume\n",
    "    df['prev_close'] = df['Close'].shift(1)\n",
    "    df['prev_open'] = df['Open'].shift(1)\n",
    "    df['prev_high'] = df['High'].shift(1)\n",
    "    df['prev_low'] = df['Low'].shift(1)\n",
    "    df['prev_volume'] = df['Volume'].shift(1)\n",
    "    \n",
    "    # ========== 1. MOMENTUM INDICATORS ==========\n",
    "    df['returns_1d'] = df['prev_close'].pct_change(1)\n",
    "    df['returns_5d'] = df['prev_close'].pct_change(5)\n",
    "    df['returns_10d'] = df['prev_close'].pct_change(10)\n",
    "    df['returns_20d'] = df['prev_close'].pct_change(20)\n",
    "    df['momentum_5d'] = df['prev_close'] - df['Close'].shift(6)\n",
    "    df['momentum_10d'] = df['prev_close'] - df['Close'].shift(11)\n",
    "    df['acceleration'] = df['returns_1d'] - df['returns_1d'].shift(1)\n",
    "    df['ROC_5'] = ((df['prev_close'] - df['Close'].shift(6)) / (df['Close'].shift(6) + 1e-8)) * 100\n",
    "    df['ROC_10'] = ((df['prev_close'] - df['Close'].shift(11)) / (df['Close'].shift(11) + 1e-8)) * 100\n",
    "    \n",
    "    # ========== 2. TREND INDICATORS ==========\n",
    "    for window in [5, 10, 20, 50]:\n",
    "        ma = df['prev_close'].rolling(window).mean()\n",
    "        df[f'MA_{window}'] = ma\n",
    "        df[f'MA_{window}_ratio'] = df['prev_close'] / (ma + 1e-8)\n",
    "    \n",
    "    ema_fast = df['prev_close'].ewm(span=5, adjust=False).mean()\n",
    "    ema_slow = df['prev_close'].ewm(span=20, adjust=False).mean()\n",
    "    df['trend_strength'] = (ema_fast - ema_slow) / (ema_slow + 1e-8)\n",
    "    \n",
    "    exp1 = df['prev_close'].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = df['prev_close'].ewm(span=26, adjust=False).mean()\n",
    "    df['MACD'] = exp1 - exp2\n",
    "    df['MACD_signal'] = df['MACD'].ewm(span=9, adjust=False).mean()\n",
    "    df['MACD_hist'] = df['MACD'] - df['MACD_signal']\n",
    "    \n",
    "    # ========== 3. VOLATILITY INDICATORS ==========\n",
    "    df['volatility_5'] = df['returns_1d'].rolling(5).std()\n",
    "    df['volatility_10'] = df['returns_1d'].rolling(10).std()\n",
    "    df['Vol_20'] = df['returns_1d'].rolling(20).std()\n",
    "    df['volatility_ratio'] = df['volatility_5'] / (df['Vol_20'] + 1e-8)\n",
    "    \n",
    "    roll_mean_20 = df['prev_close'].rolling(20).mean()\n",
    "    roll_std_20 = df['prev_close'].rolling(20).std()\n",
    "    df['bb_position'] = (df['prev_close'] - roll_mean_20) / (2 * roll_std_20 + 1e-8)\n",
    "    df['bb_width'] = (4 * roll_std_20) / (roll_mean_20 + 1e-8)\n",
    "    \n",
    "    # ATR\n",
    "    high_low = df['prev_high'] - df['prev_low']\n",
    "    high_close = np.abs(df['prev_high'] - df['prev_close'].shift(1))\n",
    "    low_close = np.abs(df['prev_low'] - df['prev_close'].shift(1))\n",
    "    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df['ATR_14'] = true_range.rolling(14).mean()\n",
    "    \n",
    "    # ========== 4. VOLUME INDICATORS ==========\n",
    "    vol_ma_10 = df['prev_volume'].rolling(10).mean()\n",
    "    df['volume_ratio'] = df['prev_volume'] / (vol_ma_10 + 1e-8)\n",
    "    df['volume_momentum'] = df['prev_volume'].pct_change(5)\n",
    "    \n",
    "    hl_range = df['prev_high'] - df['prev_low']\n",
    "    df['money_flow'] = ((2 * df['prev_close'] - df['prev_high'] - df['prev_low']) / (hl_range + 1e-8)) * df['prev_volume']\n",
    "    \n",
    "    # ========== 5. CANDLESTICK PATTERNS ==========\n",
    "    df['intraday_momentum'] = (df['prev_close'] - df['prev_open']) / (df['prev_high'] - df['prev_low'] + 1e-8)\n",
    "    df['candle_body_ratio'] = (df['prev_close'] - df['prev_open']).abs() / (df['prev_high'] - df['prev_low'] + 1e-8)\n",
    "    \n",
    "    # ========== 6. TECHNICAL INDICATORS ==========\n",
    "    # RSI\n",
    "    delta = df['prev_close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "    rs = gain / (loss + 1e-8)\n",
    "    df['RSI'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # Stochastic\n",
    "    low_14 = df['prev_low'].rolling(14).min()\n",
    "    high_14 = df['prev_high'].rolling(14).max()\n",
    "    df['stoch_k'] = 100 * (df['prev_close'] - low_14) / (high_14 - low_14 + 1e-8)\n",
    "    df['stoch_d'] = df['stoch_k'].rolling(3).mean()\n",
    "    \n",
    "    # Williams %R\n",
    "    df['williams_r'] = -100 * (high_14 - df['prev_close']) / (high_14 - low_14 + 1e-8)\n",
    "    \n",
    "    # ========== 7. MACROECONOMIC ==========\n",
    "    df['prev_dgs10'] = df['DGS10'].shift(1)\n",
    "    df['prev_gdpc1'] = df['GDPC1'].shift(1)\n",
    "    df['dgs10_change'] = df['prev_dgs10'].diff()\n",
    "    df['gdpc1_growth'] = df['prev_gdpc1'].pct_change(60)\n",
    "    \n",
    "    # ========== 8. FRACTIONAL DIFFERENTIATION ==========\n",
    "    try:\n",
    "        frac_df = frac_diff_ffd(df[['prev_close']], d=0.4)\n",
    "        df['close_frac'] = frac_df['prev_close']\n",
    "    except:\n",
    "        df['close_frac'] = df['prev_close'].diff()\n",
    "    \n",
    "    feature_count = len([c for c in df.columns if c not in ['Close', 'Open', 'High', 'Low', 'Volume', 'GDPC1', 'DGS10']])\n",
    "    print(f\"  Created {feature_count} features\")\n",
    "    \n",
    "    return df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Label Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. LABEL DEFINITION\n",
    "# ==========================================\n",
    "def make_labels_momentum(df, lookahead=5, vol_factor=0.5):\n",
    "    \"\"\"Create binary labels for classification.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STEP 3: Label Definition\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    df = df.copy()\n",
    "    df['future_ret'] = df['Close'].pct_change(lookahead).shift(-lookahead)\n",
    "    \n",
    "    if 'Vol_20' not in df.columns:\n",
    "        df['Vol_20'] = df['Close'].pct_change(1).rolling(20).std()\n",
    "    \n",
    "    dynamic_threshold = df['Vol_20'] * vol_factor\n",
    "    df['Target'] = np.where(df['future_ret'] > dynamic_threshold, 1, 0)\n",
    "    df['trade_ret'] = df['future_ret']\n",
    "    \n",
    "    df = df.dropna(subset=['Target', 'trade_ret'])\n",
    "    \n",
    "    class_dist = df['Target'].value_counts(normalize=True) * 100\n",
    "    print(f\"  Lookahead period: {lookahead} days\")\n",
    "    print(f\"  Volatility factor: {vol_factor}\")\n",
    "    print(f\"  Class distribution:\")\n",
    "    print(f\"    Class 0 (Down/Neutral): {class_dist.get(0, 0):.2f}%\")\n",
    "    print(f\"    Class 1 (Up): {class_dist.get(1, 0):.2f}%\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 SOM-based Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 5. SOM FOR FEATURE SELECTION\n",
    "# ==========================================\n",
    "class SimpleSOM:\n",
    "    \"\"\"Simple Self-Organizing Map for feature clustering.\"\"\"\n",
    "    def __init__(self, x, y, input_len, learning_rate=0.5, sigma=1.0):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.learning_rate = learning_rate\n",
    "        self.sigma = sigma\n",
    "        self.weights = np.random.random((x, y, input_len))\n",
    "    \n",
    "    def _find_bmu(self, sample):\n",
    "        dists = np.linalg.norm(self.weights - sample, axis=2)\n",
    "        return np.unravel_index(np.argmin(dists), dists.shape)\n",
    "    \n",
    "    def fit(self, data, epochs=100):\n",
    "        for epoch in range(epochs):\n",
    "            lr_t = self.learning_rate * np.exp(-epoch / epochs)\n",
    "            for sample in data:\n",
    "                bmu = self._find_bmu(sample)\n",
    "                self.weights[bmu] += lr_t * (sample - self.weights[bmu])\n",
    "    \n",
    "    def get_winner_dist(self, sample):\n",
    "        dists = np.linalg.norm(self.weights - sample, axis=2)\n",
    "        bmu_idx = np.unravel_index(np.argmin(dists), dists.shape)\n",
    "        return bmu_idx, np.min(dists)\n",
    "\n",
    "def select_features_som(df, feat_cols, keep=[], n_select_per_cluster=2):\n",
    "    \"\"\"Use SOM to cluster and select representative features.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STEP 4: SOM Feature Selection (Dimensionality Reduction)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    data = df[feat_cols].T.values\n",
    "    scaler = MinMaxScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    \n",
    "    som = SimpleSOM(4, 4, data_scaled.shape[1], learning_rate=0.5, sigma=1.5)\n",
    "    som.fit(data_scaled, epochs=100)\n",
    "    \n",
    "    clusters = {}\n",
    "    for i, f in enumerate(feat_cols):\n",
    "        node, dist = som.get_winner_dist(data_scaled[i])\n",
    "        if node not in clusters:\n",
    "            clusters[node] = []\n",
    "        clusters[node].append((f, dist))\n",
    "    \n",
    "    selected = []\n",
    "    for node, items in clusters.items():\n",
    "        items.sort(key=lambda x: x[1])\n",
    "        selected.extend([item[0] for item in items[:n_select_per_cluster]])\n",
    "    \n",
    "    final = list(set(selected + keep))\n",
    "    \n",
    "    print(f\"  Initial features: {len(feat_cols)}\")\n",
    "    print(f\"  Number of clusters: {len(clusters)}\")\n",
    "    print(f\"  Selected features: {len(final)}\")\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Multicollinearity Analysis (VIF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 6. MULTI-COLLINEARITY ANALYSIS\n",
    "# ==========================================\n",
    "def check_multicollinearity(df, features, threshold=10.0):\n",
    "    \"\"\"Calculate VIF to detect multicollinearity.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STEP 5: Multi-collinearity Analysis (VIF)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "        \n",
    "        X = df[features].copy()\n",
    "        X = X.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        X_scaled = pd.DataFrame(X_scaled, columns=features)\n",
    "        \n",
    "        vif_data = pd.DataFrame()\n",
    "        vif_data[\"Feature\"] = features\n",
    "        vif_data[\"VIF\"] = [variance_inflation_factor(X_scaled.values, i) for i in range(X_scaled.shape[1])]\n",
    "        vif_data = vif_data.sort_values('VIF', ascending=False)\n",
    "        \n",
    "        high_vif = vif_data[vif_data['VIF'] > threshold]\n",
    "        print(f\"  VIF Threshold: {threshold}\")\n",
    "        print(f\"  Features with high VIF (>{threshold}): {len(high_vif)}\")\n",
    "        \n",
    "        features_to_keep = vif_data[vif_data['VIF'] <= threshold * 2]['Feature'].tolist()\n",
    "        return features_to_keep, vif_data\n",
    "    except ImportError:\n",
    "        print(\"  Warning: statsmodels not installed. Skipping VIF analysis.\")\n",
    "        return features, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 7. OUTLIER DETECTION\n",
    "# ==========================================\n",
    "def detect_and_handle_outliers(df, features, threshold=3.0):\n",
    "    \"\"\"Detect and handle outliers using IQR with Winsorization.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STEP 6: Outlier Detection\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    df = df.copy()\n",
    "    total_outliers = 0\n",
    "    \n",
    "    for feat in features:\n",
    "        if feat not in df.columns:\n",
    "            continue\n",
    "        Q1 = df[feat].quantile(0.25)\n",
    "        Q3 = df[feat].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - threshold * IQR\n",
    "        upper = Q3 + threshold * IQR\n",
    "        \n",
    "        outliers = ((df[feat] < lower) | (df[feat] > upper)).sum()\n",
    "        total_outliers += outliers\n",
    "        \n",
    "        df.loc[df[feat] < lower, feat] = lower\n",
    "        df.loc[df[feat] > upper, feat] = upper\n",
    "    \n",
    "    print(f\"  Method: IQR with threshold={threshold}\")\n",
    "    print(f\"  Total outliers winsorized: {total_outliers}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 8. EDA\n",
    "# ==========================================\n",
    "def run_comprehensive_eda(df, features, target):\n",
    "    \"\"\"Comprehensive EDA.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STEP 7: Exploratory Data Analysis (EDA)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    target_counts = df[target].value_counts(normalize=True) * 100\n",
    "    print(f\"\\n  Target Distribution:\")\n",
    "    print(f\"    Class 0: {target_counts.get(0, 0):.2f}%\")\n",
    "    print(f\"    Class 1: {target_counts.get(1, 0):.2f}%\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    \n",
    "    # Target distribution\n",
    "    ax1 = axes[0, 0]\n",
    "    colors = ['#ff6b6b', '#4ecdc4']\n",
    "    bars = ax1.bar(['Down/Neutral', 'Up'], [target_counts.get(0, 0), target_counts.get(1, 0)], color=colors)\n",
    "    ax1.set_title('Target Distribution', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('Percentage (%)')\n",
    "    for bar, val in zip(bars, [target_counts.get(0, 0), target_counts.get(1, 0)]):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, f'{val:.1f}%', ha='center')\n",
    "    \n",
    "    # Correlation heatmap\n",
    "    ax2 = axes[0, 1]\n",
    "    valid_features = [f for f in features if f in df.columns][:15]\n",
    "    corr_matrix = df[valid_features].corr()\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=False, cmap='RdBu_r', center=0, ax=ax2)\n",
    "    ax2.set_title('Feature Correlation Heatmap', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Feature distributions\n",
    "    ax3 = axes[1, 0]\n",
    "    plot_features = [f for f in ['returns_1d', 'RSI', 'volatility_10'] if f in df.columns]\n",
    "    if plot_features:\n",
    "        df_melt = df[plot_features + [target]].melt(id_vars=target, var_name='Feature', value_name='Value')\n",
    "        sns.boxplot(data=df_melt, x='Feature', y='Value', hue=target, ax=ax3, palette='Set2')\n",
    "        ax3.set_title('Feature Distribution by Target', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Returns distribution\n",
    "    ax4 = axes[1, 1]\n",
    "    if 'returns_1d' in df.columns:\n",
    "        df[df[target] == 0]['returns_1d'].hist(ax=ax4, bins=50, alpha=0.5, label='Class 0', color='#ff6b6b')\n",
    "        df[df[target] == 1]['returns_1d'].hist(ax=ax4, bins=50, alpha=0.5, label='Class 1', color='#4ecdc4')\n",
    "        ax4.set_title('1-Day Returns by Class', fontsize=12, fontweight='bold')\n",
    "        ax4.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('eda_overview.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Pairwise scatter\n",
    "    print(\"\\n  Generating pairwise scatter plots...\")\n",
    "    scatter_features = [f for f in ['returns_1d', 'RSI', 'volatility_10', 'MACD'] if f in df.columns][:4]\n",
    "    if len(scatter_features) >= 3:\n",
    "        scatter_df = df[scatter_features + [target]].copy()\n",
    "        scatter_df[target] = scatter_df[target].astype(str)\n",
    "        g = sns.pairplot(scatter_df, hue=target, diag_kind='kde', \n",
    "                        palette={'0': '#ff6b6b', '1': '#4ecdc4'},\n",
    "                        plot_kws={'alpha': 0.5, 's': 20})\n",
    "        plt.savefig('pairwise_scatter.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 9. HYPERPARAMETER OPTIMIZATION\n",
    "# ==========================================\n",
    "def optimize_base_learners(X_train, y_train, class_weights, n_iter=20, cv=5):\n",
    "    \"\"\"Optimize hyperparameters for all base learners.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STEP 8: Hyperparameter Optimization (All Base Learners)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = {}\n",
    "    tscv = TimeSeriesSplit(n_splits=cv)\n",
    "    \n",
    "    # 1. Logistic Regression\n",
    "    print(\"\\n  [1/5] Tuning Logistic Regression...\")\n",
    "    lr_param = {'C': [0.01, 0.1, 1, 10], 'solver': ['liblinear', 'lbfgs']}\n",
    "    lr_search = RandomizedSearchCV(\n",
    "        LogisticRegression(class_weight=class_weights, random_state=42, max_iter=1000),\n",
    "        lr_param, n_iter=8, scoring='roc_auc', cv=tscv, random_state=42, n_jobs=-1\n",
    "    )\n",
    "    lr_search.fit(X_train, y_train)\n",
    "    results['lr'] = lr_search.best_params_\n",
    "    print(f\"    Best: {lr_search.best_params_}, AUC: {lr_search.best_score_:.4f}\")\n",
    "    \n",
    "    # 2. Random Forest\n",
    "    print(\"\\n  [2/5] Tuning Random Forest...\")\n",
    "    rf_param = {\n",
    "        'n_estimators': sp_randint(100, 300),\n",
    "        'max_depth': sp_randint(5, 15),\n",
    "        'min_samples_split': sp_randint(5, 20),\n",
    "        'min_samples_leaf': sp_randint(2, 10)\n",
    "    }\n",
    "    rf_search = RandomizedSearchCV(\n",
    "        RandomForestClassifier(class_weight=class_weights, random_state=42, n_jobs=-1),\n",
    "        rf_param, n_iter=n_iter, scoring='roc_auc', cv=tscv, random_state=42, n_jobs=-1\n",
    "    )\n",
    "    rf_search.fit(X_train, y_train)\n",
    "    results['rf'] = rf_search.best_params_\n",
    "    print(f\"    Best: {rf_search.best_params_}, AUC: {rf_search.best_score_:.4f}\")\n",
    "    \n",
    "    # 3. LightGBM\n",
    "    print(\"\\n  [3/5] Tuning LightGBM...\")\n",
    "    lgbm_param = {\n",
    "        'n_estimators': sp_randint(100, 300),\n",
    "        'learning_rate': sp_uniform(0.01, 0.15),\n",
    "        'max_depth': sp_randint(3, 10),\n",
    "        'num_leaves': sp_randint(20, 50),\n",
    "        'min_child_samples': sp_randint(20, 50)\n",
    "    }\n",
    "    lgbm_search = RandomizedSearchCV(\n",
    "        LGBMClassifier(class_weight=class_weights, random_state=42, verbose=-1, n_jobs=-1),\n",
    "        lgbm_param, n_iter=n_iter, scoring='roc_auc', cv=tscv, random_state=42, n_jobs=-1\n",
    "    )\n",
    "    lgbm_search.fit(X_train, y_train)\n",
    "    results['lgbm'] = lgbm_search.best_params_\n",
    "    print(f\"    Best: {lgbm_search.best_params_}, AUC: {lgbm_search.best_score_:.4f}\")\n",
    "    \n",
    "    # 4. XGBoost\n",
    "    print(\"\\n  [4/5] Tuning XGBoost...\")\n",
    "    ratio = class_weights.get(1, 1) / class_weights.get(0, 1) if class_weights else 1\n",
    "    xgb_param = {\n",
    "        'n_estimators': sp_randint(100, 300),\n",
    "        'learning_rate': sp_uniform(0.01, 0.15),\n",
    "        'max_depth': sp_randint(3, 10),\n",
    "        'min_child_weight': sp_randint(1, 10)\n",
    "    }\n",
    "    xgb_search = RandomizedSearchCV(\n",
    "        XGBClassifier(scale_pos_weight=ratio, random_state=42, n_jobs=-1, eval_metric='logloss', verbosity=0),\n",
    "        xgb_param, n_iter=n_iter, scoring='roc_auc', cv=tscv, random_state=42, n_jobs=-1\n",
    "    )\n",
    "    xgb_search.fit(X_train, y_train)\n",
    "    results['xgb'] = xgb_search.best_params_\n",
    "    print(f\"    Best: {xgb_search.best_params_}, AUC: {xgb_search.best_score_:.4f}\")\n",
    "    \n",
    "    # 5. Gradient Boosting (instead of SVC for better probability calibration)\n",
    "    print(\"\\n  [5/5] Tuning Gradient Boosting...\")\n",
    "    gb_param = {\n",
    "        'n_estimators': sp_randint(100, 300),\n",
    "        'learning_rate': sp_uniform(0.01, 0.15),\n",
    "        'max_depth': sp_randint(3, 8),\n",
    "        'min_samples_split': sp_randint(5, 20)\n",
    "    }\n",
    "    gb_search = RandomizedSearchCV(\n",
    "        GradientBoostingClassifier(random_state=42),\n",
    "        gb_param, n_iter=n_iter, scoring='roc_auc', cv=tscv, random_state=42, n_jobs=-1\n",
    "    )\n",
    "    gb_search.fit(X_train, y_train)\n",
    "    results['gb'] = gb_search.best_params_\n",
    "    print(f\"    Best: {gb_search.best_params_}, AUC: {gb_search.best_score_:.4f}\")\n",
    "    \n",
    "    print(\"\\n  Hyperparameter optimization complete.\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Blending Ensemble Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 10. BLENDING ENSEMBLE CLASS (CORRECT IMPLEMENTATION)\n",
    "# ==========================================\n",
    "class BlendingEnsemble:\n",
    "    \"\"\"\n",
    "    CORRECT Blending Ensemble Implementation.\n",
    "    \n",
    "    Architecture:\n",
    "    1. Split training data: base_train (70%) + blend_holdout (30%)\n",
    "    2. Train base learners on base_train\n",
    "    3. Generate meta-features: base learners predict on blend_holdout\n",
    "    4. Train meta-learner on meta-features\n",
    "    5. Prediction: base learners predict on test â†’ meta-learner combines\n",
    "    \n",
    "    This is DIFFERENT from Stacking which uses K-fold CV for meta-features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_learners, meta_learner, blend_ratio=0.7):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        base_learners : list of (name, estimator) tuples\n",
    "        meta_learner : estimator for combining base predictions\n",
    "        blend_ratio : float, proportion of training data for base learners\n",
    "        \"\"\"\n",
    "        self.base_learners = base_learners\n",
    "        self.meta_learner = meta_learner\n",
    "        self.blend_ratio = blend_ratio\n",
    "        self.trained_base_learners = []\n",
    "        self.trained_meta_learner = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the blending ensemble.\n",
    "        \n",
    "        Step 1: Split training data\n",
    "        Step 2: Train base learners on base_train\n",
    "        Step 3: Generate meta-features on blend_holdout\n",
    "        Step 4: Train meta-learner\n",
    "        \"\"\"\n",
    "        n_samples = len(X)\n",
    "        split_idx = int(n_samples * self.blend_ratio)\n",
    "        \n",
    "        # Step 1: Split data\n",
    "        X_base_train = X[:split_idx]\n",
    "        y_base_train = y[:split_idx]\n",
    "        X_blend_holdout = X[split_idx:]\n",
    "        y_blend_holdout = y[split_idx:]\n",
    "        \n",
    "        print(f\"    Blending split: {len(X_base_train)} base_train, {len(X_blend_holdout)} blend_holdout\")\n",
    "        \n",
    "        # Step 2: Train base learners\n",
    "        self.trained_base_learners = []\n",
    "        for name, learner in self.base_learners:\n",
    "            model = clone(learner)\n",
    "            model.fit(X_base_train, y_base_train)\n",
    "            self.trained_base_learners.append((name, model))\n",
    "        \n",
    "        # Step 3: Generate meta-features\n",
    "        meta_features = self._generate_meta_features(X_blend_holdout)\n",
    "        \n",
    "        # Step 4: Train meta-learner\n",
    "        self.trained_meta_learner = clone(self.meta_learner)\n",
    "        self.trained_meta_learner.fit(meta_features, y_blend_holdout)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _generate_meta_features(self, X):\n",
    "        \"\"\"Generate meta-features from base learner predictions.\"\"\"\n",
    "        meta_features = []\n",
    "        for name, model in self.trained_base_learners:\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                proba = model.predict_proba(X)[:, 1]\n",
    "            else:\n",
    "                proba = model.predict(X)\n",
    "            meta_features.append(proba)\n",
    "        return np.column_stack(meta_features)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict probabilities using blending.\"\"\"\n",
    "        meta_features = self._generate_meta_features(X)\n",
    "        if hasattr(self.trained_meta_learner, 'predict_proba'):\n",
    "            return self.trained_meta_learner.predict_proba(meta_features)\n",
    "        else:\n",
    "            pred = self.trained_meta_learner.predict(meta_features)\n",
    "            return np.column_stack([1 - pred, pred])\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict classes.\"\"\"\n",
    "        proba = self.predict_proba(X)\n",
    "        return (proba[:, 1] >= 0.5).astype(int)\n",
    "    \n",
    "    def get_base_predictions(self, X):\n",
    "        \"\"\"Get individual base learner predictions (for analysis).\"\"\"\n",
    "        predictions = {}\n",
    "        for name, model in self.trained_base_learners:\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                predictions[name] = model.predict_proba(X)[:, 1]\n",
    "            else:\n",
    "                predictions[name] = model.predict(X)\n",
    "        return predictions\n",
    "\n",
    "def create_blending_ensemble(optimized_params, class_weights, blend_ratio=0.7):\n",
    "    \"\"\"\n",
    "    Create a BlendingEnsemble with 5 optimized base learners.\n",
    "    \"\"\"\n",
    "    ratio = class_weights.get(1, 1) / class_weights.get(0, 1) if class_weights else 1\n",
    "    \n",
    "    # Base learners with optimized parameters\n",
    "    base_learners = [\n",
    "        ('LR', LogisticRegression(\n",
    "            **{k: v for k, v in optimized_params.get('lr', {}).items()},\n",
    "            class_weight=class_weights, random_state=42, max_iter=1000\n",
    "        )),\n",
    "        ('RF', RandomForestClassifier(\n",
    "            **optimized_params.get('rf', {}),\n",
    "            class_weight=class_weights, random_state=42, n_jobs=-1\n",
    "        )),\n",
    "        ('LGBM', LGBMClassifier(\n",
    "            **optimized_params.get('lgbm', {}),\n",
    "            class_weight=class_weights, random_state=42, verbose=-1, n_jobs=-1\n",
    "        )),\n",
    "        ('XGB', XGBClassifier(\n",
    "            **optimized_params.get('xgb', {}),\n",
    "            scale_pos_weight=ratio, random_state=42, n_jobs=-1, \n",
    "            eval_metric='logloss', verbosity=0\n",
    "        )),\n",
    "        ('GB', GradientBoostingClassifier(\n",
    "            **optimized_params.get('gb', {}),\n",
    "            random_state=42\n",
    "        ))\n",
    "    ]\n",
    "    \n",
    "    # Meta-learner\n",
    "    meta_learner = LogisticRegression(solver='lbfgs', random_state=42, max_iter=1000)\n",
    "    \n",
    "    return BlendingEnsemble(base_learners, meta_learner, blend_ratio=blend_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Rolling Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 11. ROLLING BLENDING PREDICTION\n",
    "# ==========================================\n",
    "def train_predict_rolling_blending(X_all, y_all, df_index, optimized_params,\n",
    "                                    initial_train_size=500, step_size=22,\n",
    "                                    blend_ratio=0.7, min_train_samples=200):\n",
    "    \"\"\"\n",
    "    Rolling window prediction using BLENDING ensemble.\n",
    "    \n",
    "    For each rolling window:\n",
    "    1. Use training window data\n",
    "    2. Apply blending: split into base_train (70%) and blend_holdout (30%)\n",
    "    3. Train base learners on base_train\n",
    "    4. Generate meta-features on blend_holdout\n",
    "    5. Train meta-learner\n",
    "    6. Predict on test window\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STEP 9: Rolling Blending Prediction\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  Initial train size: {initial_train_size}\")\n",
    "    print(f\"  Step size: {step_size}\")\n",
    "    print(f\"  Blend ratio: {blend_ratio:.0%} base_train / {1-blend_ratio:.0%} blend_holdout\")\n",
    "    \n",
    "    n_samples = len(y_all)\n",
    "    predictions = []\n",
    "    indices = []\n",
    "    base_predictions = []\n",
    "    actuals = []\n",
    "    skipped_windows = 0\n",
    "    \n",
    "    # Progress bar\n",
    "    total_windows = (n_samples - initial_train_size) // step_size + 1\n",
    "    \n",
    "    for start_idx in tqdm(range(initial_train_size, n_samples, step_size), \n",
    "                          desc=\"Rolling Blending\", total=total_windows):\n",
    "        end_idx = min(start_idx + step_size, n_samples)\n",
    "        \n",
    "        # Training window (expanding window)\n",
    "        train_start = max(0, start_idx - initial_train_size)\n",
    "        train_end = start_idx\n",
    "        \n",
    "        X_train_window = X_all[train_start:train_end]\n",
    "        y_train_window = y_all[train_start:train_end]\n",
    "        X_test_window = X_all[start_idx:end_idx]\n",
    "        y_test_window = y_all[start_idx:end_idx]\n",
    "        \n",
    "        # Skip if insufficient data\n",
    "        if len(X_train_window) < min_train_samples or len(X_test_window) == 0:\n",
    "            skipped_windows += 1\n",
    "            continue\n",
    "        \n",
    "        # Check class distribution\n",
    "        unique_classes = np.unique(y_train_window)\n",
    "        if len(unique_classes) < 2:\n",
    "            skipped_windows += 1\n",
    "            continue\n",
    "        \n",
    "        # Minimum samples per class\n",
    "        class_counts = np.bincount(y_train_window.astype(int))\n",
    "        if min(class_counts) < 30:  # Need enough for blending split\n",
    "            skipped_windows += 1\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Calculate class weights\n",
    "            cw = dict(zip(unique_classes,\n",
    "                         class_weight.compute_class_weight('balanced',\n",
    "                                                          classes=unique_classes,\n",
    "                                                          y=y_train_window)))\n",
    "            \n",
    "            # Create and train blending ensemble\n",
    "            blending_clf = create_blending_ensemble(optimized_params, cw, blend_ratio=blend_ratio)\n",
    "            blending_clf.fit(X_train_window, y_train_window)\n",
    "            \n",
    "            # Predict\n",
    "            pred_proba = blending_clf.predict_proba(X_test_window)[:, 1]\n",
    "            predictions.extend(pred_proba)\n",
    "            indices.extend(range(start_idx, end_idx))\n",
    "            actuals.extend(y_test_window)\n",
    "            \n",
    "            # Baseline (simple LR trained on full training window)\n",
    "            lr_baseline = LogisticRegression(solver='liblinear', class_weight=cw,\n",
    "                                            random_state=42, max_iter=1000)\n",
    "            lr_baseline.fit(X_train_window, y_train_window)\n",
    "            baseline_pred = lr_baseline.predict_proba(X_test_window)[:, 1]\n",
    "            base_predictions.extend(baseline_pred)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n  Warning: Window failed: {str(e)[:50]}\")\n",
    "            skipped_windows += 1\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n  Total predictions: {len(predictions)}\")\n",
    "    print(f\"  Skipped windows: {skipped_windows}\")\n",
    "    \n",
    "    return (np.array(predictions), np.array(indices),\n",
    "            np.array(base_predictions), np.array(actuals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 12. EVALUATION FUNCTIONS\n",
    "# ==========================================\n",
    "def evaluate_model(y_true, y_pred_prob, y_pred_binary, model_name=\"Model\"):\n",
    "    \"\"\"Comprehensive model evaluation.\"\"\"\n",
    "    results = {\n",
    "        'AUC': roc_auc_score(y_true, y_pred_prob),\n",
    "        'Balanced_Accuracy': balanced_accuracy_score(y_true, y_pred_binary),\n",
    "        'MCC': matthews_corrcoef(y_true, y_pred_binary),\n",
    "        'F1': f1_score(y_true, y_pred_binary),\n",
    "        'Brier': brier_score_loss(y_true, y_pred_prob),\n",
    "        'AP': average_precision_score(y_true, y_pred_prob)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n  {model_name} Evaluation:\")\n",
    "    print(f\"    AUC-ROC: {results['AUC']:.4f}\")\n",
    "    print(f\"    Average Precision: {results['AP']:.4f}\")\n",
    "    print(f\"    Balanced Accuracy: {results['Balanced_Accuracy']:.4f}\")\n",
    "    print(f\"    MCC: {results['MCC']:.4f}\")\n",
    "    print(f\"    F1 Score: {results['F1']:.4f}\")\n",
    "    print(f\"    Brier Score: {results['Brier']:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_confusion_matrix_comparison(y_true, y_pred_ensemble, y_pred_baseline):\n",
    "    \"\"\"Plot confusion matrices.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    cm_ens = confusion_matrix(y_true, y_pred_ensemble)\n",
    "    ConfusionMatrixDisplay(cm_ens, display_labels=['Down', 'Up']).plot(ax=axes[0], cmap='Blues')\n",
    "    axes[0].set_title('Blending Ensemble', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    cm_base = confusion_matrix(y_true, y_pred_baseline)\n",
    "    ConfusionMatrixDisplay(cm_base, display_labels=['Down', 'Up']).plot(ax=axes[1], cmap='Oranges')\n",
    "    axes[1].set_title('Baseline (Logistic Regression)', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrices.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 13. BACKTESTING (CORRECTED)\n",
    "# ==========================================\n",
    "def calculate_backtest_metrics(returns, name=\"Strategy\"):\n",
    "    \"\"\"Calculate backtest metrics with CORRECT max drawdown.\"\"\"\n",
    "    active_returns = returns[returns != 0]\n",
    "    \n",
    "    if len(active_returns) < 10:\n",
    "        return {'Sharpe': 0, 'Sortino': 0, 'Total_Return': 0,\n",
    "                'Max_Drawdown': 0, 'Win_Rate': 0, 'Num_Trades': 0, 'Profit_Factor': 0}\n",
    "    \n",
    "    ann_factor = np.sqrt(252 / 5)\n",
    "    \n",
    "    # Sharpe\n",
    "    sharpe = np.mean(active_returns) / (np.std(active_returns) + 1e-9) * ann_factor\n",
    "    \n",
    "    # Sortino\n",
    "    downside = active_returns[active_returns < 0]\n",
    "    sortino = np.mean(active_returns) / (np.std(downside) + 1e-9) * ann_factor if len(downside) > 0 else sharpe\n",
    "    \n",
    "    # Total Return\n",
    "    total_return = (1 + returns).prod() - 1\n",
    "    \n",
    "    # CORRECTED Max Drawdown\n",
    "    cumulative = (1 + returns).cumprod()\n",
    "    peak = np.maximum.accumulate(cumulative)\n",
    "    drawdown = (peak - cumulative) / (peak + 1e-9)\n",
    "    max_drawdown = np.max(drawdown)\n",
    "    \n",
    "    # Win Rate\n",
    "    win_rate = np.mean(active_returns > 0)\n",
    "    \n",
    "    # Profit Factor\n",
    "    gross_profit = np.sum(active_returns[active_returns > 0])\n",
    "    gross_loss = np.abs(np.sum(active_returns[active_returns < 0]))\n",
    "    profit_factor = gross_profit / (gross_loss + 1e-9)\n",
    "    \n",
    "    return {\n",
    "        'Sharpe': sharpe, 'Sortino': sortino, 'Total_Return': total_return,\n",
    "        'Max_Drawdown': max_drawdown, 'Win_Rate': win_rate,\n",
    "        'Num_Trades': len(active_returns), 'Profit_Factor': profit_factor\n",
    "    }\n",
    "\n",
    "def run_backtest(test_dates, test_returns, y_pred_prob, y_baseline_prob,\n",
    "                 threshold=0.5, apply_trend_filter=True, df=None, aligned_indices=None):\n",
    "    \"\"\"Run backtest.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STEP 10: Backtesting\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    signal_ensemble = (y_pred_prob >= threshold).astype(int)\n",
    "    signal_baseline = (y_baseline_prob >= threshold).astype(int)\n",
    "    \n",
    "    ret_ensemble = test_returns * signal_ensemble\n",
    "    ret_baseline = test_returns * signal_baseline\n",
    "    ret_market = test_returns.copy()\n",
    "    \n",
    "    if apply_trend_filter and df is not None and aligned_indices is not None:\n",
    "        ma200 = df['Close'].rolling(200).mean().iloc[aligned_indices].values\n",
    "        price = df['Close'].iloc[aligned_indices].values\n",
    "        min_len = min(len(ma200), len(ret_ensemble))\n",
    "        trend_filter = (price[:min_len] > ma200[:min_len]).astype(int)\n",
    "        ret_ensemble[:min_len] *= trend_filter\n",
    "        ret_baseline[:min_len] *= trend_filter\n",
    "        print(f\"  Applied 200-day MA trend filter (uptrend: {np.mean(trend_filter)*100:.1f}%)\")\n",
    "    \n",
    "    metrics_ens = calculate_backtest_metrics(ret_ensemble, \"Ensemble\")\n",
    "    metrics_base = calculate_backtest_metrics(ret_baseline, \"Baseline\")\n",
    "    metrics_mkt = calculate_backtest_metrics(ret_market, \"Market\")\n",
    "    \n",
    "    print(f\"\\n  Performance Summary:\")\n",
    "    print(f\"  {'Metric':<18} {'Ensemble':>12} {'Baseline':>12} {'Market':>12}\")\n",
    "    print(f\"  {'-'*54}\")\n",
    "    print(f\"  {'Sharpe Ratio':<18} {metrics_ens['Sharpe']:>12.3f} {metrics_base['Sharpe']:>12.3f} {metrics_mkt['Sharpe']:>12.3f}\")\n",
    "    print(f\"  {'Sortino Ratio':<18} {metrics_ens['Sortino']:>12.3f} {metrics_base['Sortino']:>12.3f} {metrics_mkt['Sortino']:>12.3f}\")\n",
    "    print(f\"  {'Total Return':<18} {metrics_ens['Total_Return']:>11.2%} {metrics_base['Total_Return']:>11.2%} {metrics_mkt['Total_Return']:>11.2%}\")\n",
    "    print(f\"  {'Max Drawdown':<18} {metrics_ens['Max_Drawdown']:>11.2%} {metrics_base['Max_Drawdown']:>11.2%} {metrics_mkt['Max_Drawdown']:>11.2%}\")\n",
    "    print(f\"  {'Win Rate':<18} {metrics_ens['Win_Rate']:>11.2%} {metrics_base['Win_Rate']:>11.2%} {metrics_mkt['Win_Rate']:>11.2%}\")\n",
    "    print(f\"  {'Profit Factor':<18} {metrics_ens['Profit_Factor']:>12.2f} {metrics_base['Profit_Factor']:>12.2f} {metrics_mkt['Profit_Factor']:>12.2f}\")\n",
    "    \n",
    "    return ret_ensemble, ret_baseline, ret_market, metrics_ens, metrics_base\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 14. VISUALIZATION FUNCTIONS\n",
    "# ==========================================\n",
    "def plot_cumulative_returns(test_dates, ret_ensemble, ret_baseline, ret_market):\n",
    "    \"\"\"Plot cumulative returns.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "    \n",
    "    # Additive cumulative\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(test_dates, np.cumsum(ret_market), label='Market', color='gray', alpha=0.6)\n",
    "    ax1.plot(test_dates, np.cumsum(ret_baseline), label='Baseline (LR)', color='orange', linestyle='--')\n",
    "    ax1.plot(test_dates, np.cumsum(ret_ensemble), label='Blending Ensemble', color='blue', linewidth=2)\n",
    "    ax1.set_title('Cumulative Returns (Additive)', fontsize=14, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "    \n",
    "    # Multiplicative (wealth)\n",
    "    ax2 = axes[1]\n",
    "    ax2.plot(test_dates, (1 + ret_market).cumprod(), label='Market', color='gray', alpha=0.6)\n",
    "    ax2.plot(test_dates, (1 + ret_baseline).cumprod(), label='Baseline', color='orange', linestyle='--')\n",
    "    ax2.plot(test_dates, (1 + ret_ensemble).cumprod(), label='Blending Ensemble', color='blue', linewidth=2)\n",
    "    ax2.set_title('Portfolio Value ($1 Initial)', fontsize=14, fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cumulative_returns.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_drawdown(test_dates, ret_ensemble, ret_baseline, ret_market):\n",
    "    \"\"\"Plot drawdown comparison.\"\"\"\n",
    "    def calc_dd(returns):\n",
    "        cum = (1 + returns).cumprod()\n",
    "        peak = np.maximum.accumulate(cum)\n",
    "        return (peak - cum) / peak\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.fill_between(test_dates, 0, -calc_dd(ret_market), alpha=0.3, color='gray', label='Market')\n",
    "    plt.fill_between(test_dates, 0, -calc_dd(ret_baseline), alpha=0.3, color='orange', label='Baseline')\n",
    "    plt.fill_between(test_dates, 0, -calc_dd(ret_ensemble), alpha=0.5, color='blue', label='Ensemble')\n",
    "    plt.title('Drawdown Comparison', fontsize=14, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('drawdown_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_rolling_metrics(test_dates, ret_ensemble, ret_baseline, ret_market, window=63):\n",
    "    \"\"\"Plot rolling metrics.\"\"\"\n",
    "    ann_factor = np.sqrt(252 / 5)\n",
    "    ens = pd.Series(ret_ensemble, index=test_dates)\n",
    "    base = pd.Series(ret_baseline, index=test_dates)\n",
    "    mkt = pd.Series(ret_market, index=test_dates)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "    \n",
    "    # Rolling Sharpe\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(mkt.rolling(window).mean() / (mkt.rolling(window).std() + 1e-9) * ann_factor,\n",
    "             label='Market', color='gray', alpha=0.5)\n",
    "    ax1.plot(base.rolling(window).mean() / (base.rolling(window).std() + 1e-9) * ann_factor,\n",
    "             label='Baseline', color='orange', linestyle='--')\n",
    "    ax1.plot(ens.rolling(window).mean() / (ens.rolling(window).std() + 1e-9) * ann_factor,\n",
    "             label='Ensemble', color='blue')\n",
    "    ax1.axhline(0, color='red', linestyle=':', alpha=0.5)\n",
    "    ax1.set_title(f'Rolling Sharpe Ratio ({window}-day)', fontsize=12, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Rolling Win Rate\n",
    "    ax2 = axes[1]\n",
    "    ax2.plot((mkt > 0).rolling(window).mean(), label='Market', color='gray', alpha=0.5)\n",
    "    ax2.plot((base > 0).rolling(window).mean(), label='Baseline', color='orange', linestyle='--')\n",
    "    ax2.plot((ens > 0).rolling(window).mean(), label='Ensemble', color='blue')\n",
    "    ax2.axhline(0.5, color='red', linestyle=':', alpha=0.5)\n",
    "    ax2.set_title(f'Rolling Win Rate ({window}-day)', fontsize=12, fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('rolling_metrics.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_pr_curves(y_true, y_pred_ensemble, y_pred_baseline):\n",
    "    \"\"\"Plot ROC and PR curves.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # ROC\n",
    "    ax1 = axes[0]\n",
    "    fpr_e, tpr_e, _ = roc_curve(y_true, y_pred_ensemble)\n",
    "    fpr_b, tpr_b, _ = roc_curve(y_true, y_pred_baseline)\n",
    "    auc_e = roc_auc_score(y_true, y_pred_ensemble)\n",
    "    auc_b = roc_auc_score(y_true, y_pred_baseline)\n",
    "    ax1.plot(fpr_e, tpr_e, label=f'Ensemble (AUC={auc_e:.4f})', color='blue', linewidth=2)\n",
    "    ax1.plot(fpr_b, tpr_b, label=f'Baseline (AUC={auc_b:.4f})', color='orange', linestyle='--')\n",
    "    ax1.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    ax1.set_xlabel('FPR')\n",
    "    ax1.set_ylabel('TPR')\n",
    "    ax1.set_title('ROC Curve', fontsize=12, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # PR\n",
    "    ax2 = axes[1]\n",
    "    prec_e, rec_e, _ = precision_recall_curve(y_true, y_pred_ensemble)\n",
    "    prec_b, rec_b, _ = precision_recall_curve(y_true, y_pred_baseline)\n",
    "    ap_e = average_precision_score(y_true, y_pred_ensemble)\n",
    "    ap_b = average_precision_score(y_true, y_pred_baseline)\n",
    "    ax2.plot(rec_e, prec_e, label=f'Ensemble (AP={ap_e:.4f})', color='blue', linewidth=2)\n",
    "    ax2.plot(rec_b, prec_b, label=f'Baseline (AP={ap_b:.4f})', color='orange', linestyle='--')\n",
    "    ax2.axhline(np.mean(y_true), color='gray', linestyle=':', label=f'Baseline ({np.mean(y_true):.2%})')\n",
    "    ax2.set_xlabel('Recall')\n",
    "    ax2.set_ylabel('Precision')\n",
    "    ax2.set_title('Precision-Recall Curve', fontsize=12, fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('roc_pr_curves.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_feature_importance(X_train, y_train, feature_names, top_n=15):\n",
    "    \"\"\"Plot feature importance.\"\"\"\n",
    "    print(\"\\n  Generating feature importance plot...\")\n",
    "    rf = RandomForestClassifier(n_estimators=100, max_depth=8, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': rf.feature_importances_\n",
    "    }).sort_values('Importance', ascending=True).tail(top_n)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(importance['Feature'], importance['Importance'], color='steelblue')\n",
    "    plt.title('Feature Importance (Random Forest)', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def generate_summary_table(eval_ens, eval_base, metrics_ens, metrics_base):\n",
    "    \"\"\"Generate summary table.\"\"\"\n",
    "    summary = pd.DataFrame({\n",
    "        'Metric': ['AUC-ROC', 'Average Precision', 'Balanced Accuracy', 'MCC', 'F1 Score', 'Brier Score',\n",
    "                   'Sharpe Ratio', 'Sortino Ratio', 'Total Return', 'Max Drawdown', 'Win Rate', 'Profit Factor'],\n",
    "        'Blending Ensemble': [\n",
    "            f\"{eval_ens['AUC']:.4f}\", f\"{eval_ens['AP']:.4f}\", f\"{eval_ens['Balanced_Accuracy']:.4f}\",\n",
    "            f\"{eval_ens['MCC']:.4f}\", f\"{eval_ens['F1']:.4f}\", f\"{eval_ens['Brier']:.4f}\",\n",
    "            f\"{metrics_ens['Sharpe']:.3f}\", f\"{metrics_ens['Sortino']:.3f}\", f\"{metrics_ens['Total_Return']:.2%}\",\n",
    "            f\"{metrics_ens['Max_Drawdown']:.2%}\", f\"{metrics_ens['Win_Rate']:.2%}\", f\"{metrics_ens['Profit_Factor']:.2f}\"\n",
    "        ],\n",
    "        'Baseline (LR)': [\n",
    "            f\"{eval_base['AUC']:.4f}\", f\"{eval_base['AP']:.4f}\", f\"{eval_base['Balanced_Accuracy']:.4f}\",\n",
    "            f\"{eval_base['MCC']:.4f}\", f\"{eval_base['F1']:.4f}\", f\"{eval_base['Brier']:.4f}\",\n",
    "            f\"{metrics_base['Sharpe']:.3f}\", f\"{metrics_base['Sortino']:.3f}\", f\"{metrics_base['Total_Return']:.2%}\",\n",
    "            f\"{metrics_base['Max_Drawdown']:.2%}\", f\"{metrics_base['Win_Rate']:.2%}\", f\"{metrics_base['Profit_Factor']:.2f}\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SUMMARY TABLE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(summary.to_string(index=False))\n",
    "    return summary\n",
    "\n",
    "def create_methods_table():\n",
    "    \"\"\"Create numerical methods table for report.\"\"\"\n",
    "    methods = pd.DataFrame({\n",
    "        'Method': ['Self-Organizing Map (SOM)', 'Fractional Differentiation', 'VIF Analysis',\n",
    "                   'Logistic Regression', 'Random Forest', 'LightGBM', 'XGBoost', 'Gradient Boosting',\n",
    "                   'Blending Ensemble', 'RandomizedSearchCV', 'TimeSeriesSplit'],\n",
    "        'Purpose': ['Feature clustering & selection', 'Stationarity with memory', 'Multicollinearity detection',\n",
    "                    'Base learner 1', 'Base learner 2', 'Base learner 3', 'Base learner 4', 'Base learner 5',\n",
    "                    'Meta-learner combination', 'Hyperparameter tuning', 'Temporal cross-validation'],\n",
    "        'Implementation': ['Custom class', 'Custom function', 'statsmodels', 'sklearn', 'sklearn',\n",
    "                          'lightgbm', 'xgboost', 'sklearn', 'Custom BlendingEnsemble class', 'sklearn', 'sklearn']\n",
    "    })\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"NUMERICAL METHODS TABLE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(methods.to_string(index=False))\n",
    "    return methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Main Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1 Main Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 15. MAIN EXECUTION\n",
    "# ==========================================\n",
    "def run_final_project():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"CQF FINAL PROJECT: BLENDING ENSEMBLE FOR CLASSIFICATION\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Asset: SPY ETF\")\n",
    "    print(\"Prediction: 5-day forward return direction\")\n",
    "    print(\"Method: BLENDING Ensemble with 5 base learners\")\n",
    "    print(\"        (NOT Stacking - uses holdout split, not K-fold CV)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # File paths\n",
    "    spy_train_path = 'SPYtrain.csv'\n",
    "    spy_test_path = 'SPYtest.csv'\n",
    "    gdp_path = 'GDPC1.csv'\n",
    "    dgs10_path = 'DGS10.csv'\n",
    "    \n",
    "    # STEP 1: Load Data\n",
    "    df, oos_start_date = load_and_merge(spy_train_path, spy_test_path, gdp_path, dgs10_path)\n",
    "    \n",
    "    # STEP 2: Feature Engineering\n",
    "    df = create_features_enhanced(df)\n",
    "    \n",
    "    # STEP 3: Label Definition\n",
    "    df = make_labels_momentum(df, lookahead=5, vol_factor=0.5)\n",
    "    \n",
    "    # STEP 4: Feature Selection\n",
    "    drop_cols = ['Target', 'Close', 'Open', 'High', 'Low', 'Volume', 'GDPC1', 'DGS10',\n",
    "                 'trade_ret', 'future_ret', 'prev_close', 'prev_open', 'prev_high', 'prev_low', 'prev_volume']\n",
    "    feats_all = [c for c in df.columns if c not in drop_cols]\n",
    "    \n",
    "    core_feats = ['returns_1d', 'returns_5d', 'momentum_5d', 'volatility_10',\n",
    "                  'money_flow', 'RSI', 'MACD', 'trend_strength', 'close_frac', 'bb_position']\n",
    "    core_feats = [f for f in core_feats if f in feats_all]\n",
    "    \n",
    "    sel_feats = select_features_som(df, feats_all, keep=core_feats, n_select_per_cluster=2)\n",
    "    \n",
    "    # STEP 5: VIF Analysis\n",
    "    sel_feats, vif_results = check_multicollinearity(df, sel_feats, threshold=10.0)\n",
    "    \n",
    "    # STEP 6: Outlier Detection\n",
    "    df = detect_and_handle_outliers(df, sel_feats, threshold=3.0)\n",
    "    \n",
    "    # STEP 7: EDA\n",
    "    run_comprehensive_eda(df.copy(), sel_feats, 'Target')\n",
    "    \n",
    "    # STEP 8: Data Preparation\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Data Preparation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    oos_idx = df.index.get_indexer([oos_start_date], method='nearest')[0]\n",
    "    \n",
    "    X_all = df[sel_feats].values\n",
    "    y_all = df['Target'].values\n",
    "    \n",
    "    X_train = X_all[:oos_idx]\n",
    "    y_train = y_all[:oos_idx]\n",
    "    X_test = X_all[oos_idx:]\n",
    "    y_test = y_all[oos_idx:]\n",
    "    \n",
    "    print(f\"  Training samples: {len(X_train)}\")\n",
    "    print(f\"  Test samples: {len(X_test)}\")\n",
    "    print(f\"  Features: {len(sel_feats)}\")\n",
    "    print(f\"  Train class dist: {np.bincount(y_train.astype(int))}\")\n",
    "    \n",
    "    # Scale data (fit on train only!)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_all_scaled = np.vstack([X_train_scaled, X_test_scaled])\n",
    "    y_all_combined = np.concatenate([y_train, y_test])\n",
    "    \n",
    "    # STEP 9: Hyperparameter Optimization\n",
    "    classes = np.unique(y_train)\n",
    "    class_weights = dict(zip(classes, class_weight.compute_class_weight('balanced', classes=classes, y=y_train)))\n",
    "    print(f\"\\n  Class weights: {class_weights}\")\n",
    "    \n",
    "    optimized_params = optimize_base_learners(X_train_scaled, y_train, class_weights, n_iter=20, cv=5)\n",
    "    \n",
    "    # STEP 10: Rolling Blending Prediction\n",
    "    INITIAL_TRAIN_SIZE = len(X_train)\n",
    "    STEP_SIZE = 22\n",
    "    BLEND_RATIO = 0.7  # 70% base_train, 30% blend_holdout\n",
    "    MIN_TRAIN = min(200, len(X_train) // 3)\n",
    "    \n",
    "    y_pred_prob, test_indices, baseline_prob, y_test_rolling = train_predict_rolling_blending(\n",
    "        X_all_scaled, y_all_combined, df.index,\n",
    "        optimized_params,\n",
    "        initial_train_size=INITIAL_TRAIN_SIZE,\n",
    "        step_size=STEP_SIZE,\n",
    "        blend_ratio=BLEND_RATIO,\n",
    "        min_train_samples=MIN_TRAIN\n",
    "    )\n",
    "    \n",
    "    # STEP 11: Evaluation\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Model Evaluation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Find optimal threshold\n",
    "    prec, rec, thresholds = precision_recall_curve(y_test_rolling, y_pred_prob)\n",
    "    f1_scores = 2 * prec * rec / (prec + rec + 1e-9)\n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    optimal_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 0.5\n",
    "    optimal_threshold = np.clip(optimal_threshold, 0.3, 0.7)\n",
    "    print(f\"\\n  Optimal threshold: {optimal_threshold:.4f}\")\n",
    "    \n",
    "    y_pred_binary = (y_pred_prob >= optimal_threshold).astype(int)\n",
    "    baseline_binary = (baseline_prob >= optimal_threshold).astype(int)\n",
    "    \n",
    "    eval_ensemble = evaluate_model(y_test_rolling, y_pred_prob, y_pred_binary, \"Blending Ensemble\")\n",
    "    eval_baseline = evaluate_model(y_test_rolling, baseline_prob, baseline_binary, \"Baseline (LR)\")\n",
    "    \n",
    "    print(\"\\n  Classification Report (Ensemble):\")\n",
    "    print(classification_report(y_test_rolling, y_pred_binary, target_names=['Down', 'Up']))\n",
    "    \n",
    "    plot_confusion_matrix_comparison(y_test_rolling, y_pred_binary, baseline_binary)\n",
    "    \n",
    "    # STEP 12: Backtesting\n",
    "    aligned_indices = test_indices\n",
    "    test_returns = df['trade_ret'].iloc[aligned_indices].values\n",
    "    test_dates = df.index[aligned_indices]\n",
    "    \n",
    "    min_len = min(len(test_returns), len(y_pred_binary))\n",
    "    test_returns = test_returns[:min_len]\n",
    "    test_dates = test_dates[:min_len]\n",
    "    y_pred_prob_bt = y_pred_prob[:min_len]\n",
    "    baseline_prob_bt = baseline_prob[:min_len]\n",
    "    y_test_bt = y_test_rolling[:min_len]\n",
    "    \n",
    "    ret_ens, ret_base, ret_mkt, metrics_ens, metrics_base = run_backtest(\n",
    "        test_dates, test_returns, y_pred_prob_bt, baseline_prob_bt,\n",
    "        threshold=optimal_threshold, apply_trend_filter=True,\n",
    "        df=df, aligned_indices=aligned_indices[:min_len]\n",
    "    )\n",
    "    \n",
    "    # STEP 13: Visualizations\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Generating Visualizations\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    plot_cumulative_returns(test_dates, ret_ens, ret_base, ret_mkt)\n",
    "    plot_drawdown(test_dates, ret_ens, ret_base, ret_mkt)\n",
    "    plot_rolling_metrics(test_dates, ret_ens, ret_base, ret_mkt, window=63)\n",
    "    plot_roc_pr_curves(y_test_bt, y_pred_prob_bt, baseline_prob_bt)\n",
    "    plot_feature_importance(X_train_scaled, y_train, sel_feats, top_n=15)\n",
    "    \n",
    "    # STEP 14: Summary\n",
    "    summary = generate_summary_table(eval_ensemble, eval_baseline, metrics_ens, metrics_base)\n",
    "    summary.to_csv('model_comparison_summary.csv', index=False)\n",
    "    \n",
    "    methods = create_methods_table()\n",
    "    methods.to_csv('numerical_methods.csv', index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"PROJECT COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nKey Implementation Details:\")\n",
    "    print(\"  - BLENDING (not Stacking): Training data split into base_train + blend_holdout\")\n",
    "    print(\"  - 5 Base Learners: LR, RF, LightGBM, XGBoost, Gradient Boosting\")\n",
    "    print(\"  - Meta-Learner: Logistic Regression\")\n",
    "    print(\"  - All hyperparameters optimized via RandomizedSearchCV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 Run the Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    run_final_project()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Additional Analysis: Signal-to-Noise Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# ADDITIONAL PLOTS - Standalone Version\n",
    "# ==========================================\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ADDITIONAL ANALYSIS - Loading Data\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Reload data\n",
    "spy_train_path = 'SPYtrain.csv'\n",
    "spy_test_path = 'SPYtest.csv'\n",
    "gdp_path = 'GDPC1.csv'\n",
    "dgs10_path = 'DGS10.csv'\n",
    "\n",
    "df_train = pd.read_csv(spy_train_path, parse_dates=['Date'], index_col='Date').sort_index()\n",
    "df_test = pd.read_csv(spy_test_path, parse_dates=['Date'], index_col='Date').sort_index()\n",
    "oos_start_date = df_test.index.min()\n",
    "df = pd.concat([df_train, df_test]).sort_index()\n",
    "\n",
    "for col in ['Close/Last', 'Open', 'High', 'Low']:\n",
    "    if col in df.columns and df[col].dtype == 'object':\n",
    "        df[col] = df[col].str.replace('$', '').astype(float)\n",
    "if 'Close/Last' in df.columns:\n",
    "    df.rename(columns={'Close/Last': 'Close'}, inplace=True)\n",
    "\n",
    "gdp = pd.read_csv(gdp_path, parse_dates=['observation_date'])\n",
    "gdp.columns = ['Date', 'GDPC1']\n",
    "gdp.set_index('Date', inplace=True)\n",
    "dgs10 = pd.read_csv(dgs10_path, parse_dates=['observation_date'])\n",
    "dgs10.columns = ['Date', 'DGS10']\n",
    "dgs10['DGS10'] = pd.to_numeric(dgs10['DGS10'], errors='coerce')\n",
    "dgs10.set_index('Date', inplace=True)\n",
    "df = df.join(gdp.reindex(df.index, method='ffill'))\n",
    "df = df.join(dgs10.reindex(df.index, method='ffill')).ffill().bfill()\n",
    "\n",
    "# Create features\n",
    "df['prev_close'] = df['Close'].shift(1)\n",
    "df['returns_1d'] = df['prev_close'].pct_change(1)\n",
    "df['returns_5d'] = df['prev_close'].pct_change(5)\n",
    "df['returns_10d'] = df['prev_close'].pct_change(10)\n",
    "df['returns_20d'] = df['prev_close'].pct_change(20)\n",
    "df['momentum_5d'] = df['prev_close'] - df['Close'].shift(6)\n",
    "df['volatility_5'] = df['returns_1d'].rolling(5).std()\n",
    "df['volatility_10'] = df['returns_1d'].rolling(10).std()\n",
    "df['Vol_20'] = df['returns_1d'].rolling(20).std()\n",
    "\n",
    "for window in [5, 10, 20, 50]:\n",
    "    ma = df['prev_close'].rolling(window).mean()\n",
    "    df[f'MA_{window}_ratio'] = df['prev_close'] / (ma + 1e-8)\n",
    "\n",
    "exp1 = df['prev_close'].ewm(span=12, adjust=False).mean()\n",
    "exp2 = df['prev_close'].ewm(span=26, adjust=False).mean()\n",
    "df['MACD'] = exp1 - exp2\n",
    "\n",
    "delta = df['prev_close'].diff()\n",
    "gain = (delta.where(delta > 0, 0)).rolling(14).mean()\n",
    "loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "df['RSI'] = 100 - (100 / (1 + gain / (loss + 1e-8)))\n",
    "\n",
    "roll_mean = df['prev_close'].rolling(20).mean()\n",
    "roll_std = df['prev_close'].rolling(20).std()\n",
    "df['bb_position'] = (df['prev_close'] - roll_mean) / (2 * roll_std + 1e-8)\n",
    "\n",
    "df['future_ret'] = df['Close'].pct_change(5).shift(-5)\n",
    "df['Target'] = np.where(df['future_ret'] > df['Vol_20'] * 0.5, 1, 0)\n",
    "df = df.dropna()\n",
    "\n",
    "features = ['returns_1d', 'returns_5d', 'returns_10d', 'returns_20d',\n",
    "            'momentum_5d', 'volatility_5', 'volatility_10', 'Vol_20',\n",
    "            'MA_5_ratio', 'MA_10_ratio', 'MA_20_ratio', 'MA_50_ratio',\n",
    "            'MACD', 'RSI', 'bb_position']\n",
    "features = [f for f in features if f in df.columns]\n",
    "\n",
    "oos_idx = df.index.get_indexer([oos_start_date], method='nearest')[0]\n",
    "X_train = df[features].values[:oos_idx]\n",
    "y_train = df['Target'].values[:oos_idx]\n",
    "X_test = df[features].values[oos_idx:]\n",
    "y_test = df['Target'].values[oos_idx:]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "classes = np.unique(y_train)\n",
    "class_weights = dict(zip(classes, class_weight.compute_class_weight('balanced', classes=classes, y=y_train)))\n",
    "\n",
    "print(f\"  Train: {len(X_train)}, Test: {len(X_test)}, Features: {len(features)}\")\n",
    "\n",
    "# ==========================================\n",
    "# PLOT 1: Base Learners Comparison\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Plot 1: Base Learners Individual Performance\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "ratio = class_weights.get(1, 1) / class_weights.get(0, 1)\n",
    "learners = {\n",
    "    'Logistic Regression': LogisticRegression(class_weight=class_weights, random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=200, max_depth=10, class_weight=class_weights, random_state=42, n_jobs=-1),\n",
    "    'LightGBM': LGBMClassifier(n_estimators=200, max_depth=6, class_weight=class_weights, random_state=42, verbose=-1, n_jobs=-1),\n",
    "    'XGBoost': XGBClassifier(n_estimators=200, max_depth=6, scale_pos_weight=ratio, random_state=42, n_jobs=-1, eval_metric='logloss', verbosity=0),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=200, max_depth=5, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "predictions = {}\n",
    "for name, model in learners.items():\n",
    "    print(f\"  Training {name}...\")\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    auc = roc_auc_score(y_test, y_pred_prob)\n",
    "    results[name] = auc\n",
    "    predictions[name] = y_pred_prob\n",
    "    print(f\"    AUC: {auc:.4f}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1 = axes[0]\n",
    "names = list(results.keys())\n",
    "aucs = list(results.values())\n",
    "colors = ['#3498db', '#2ecc71', '#9b59b6', '#e74c3c', '#f39c12']\n",
    "bars = ax1.barh(names, aucs, color=colors, edgecolor='black', linewidth=0.5)\n",
    "ax1.axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Random (0.5)')\n",
    "ax1.set_xlim(0.45, 0.60)\n",
    "ax1.set_xlabel('AUC-ROC', fontsize=11)\n",
    "ax1.set_title('Base Learners Individual AUC', fontsize=12, fontweight='bold')\n",
    "for bar, auc_val in zip(bars, aucs):\n",
    "    ax1.text(auc_val + 0.005, bar.get_y() + bar.get_height()/2, f'{auc_val:.4f}', va='center')\n",
    "ax1.legend(loc='lower right')\n",
    "\n",
    "ax2 = axes[1]\n",
    "for (name, y_pred), color in zip(predictions.items(), colors):\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "    ax2.plot(fpr, tpr, label=f'{name} ({results[name]:.4f})', color=color, linewidth=2)\n",
    "ax2.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "ax2.set_xlabel('FPR'); ax2.set_ylabel('TPR')\n",
    "ax2.set_title('ROC Curves - All Base Learners', fontsize=12, fontweight='bold')\n",
    "ax2.legend(loc='lower right', fontsize=9); ax2.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('base_learners_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap\n",
    "pred_df = pd.DataFrame(predictions)\n",
    "corr_matrix = pred_df.corr()\n",
    "print(\"\\n  Prediction Correlation Matrix:\")\n",
    "print(corr_matrix.round(3))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='RdYlBu_r', center=0.5, vmin=0, vmax=1, fmt='.3f')\n",
    "plt.title('Base Learners Prediction Correlation\\n(High = Low diversity)', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('base_learners_correlation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ==========================================\n",
    "# PLOT 2: Signal-to-Noise Analysis\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Plot 2: Signal-to-Noise Ratio Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "correlations = []\n",
    "for feat in features:\n",
    "    pearson_r, _ = pearsonr(df[feat].fillna(0), df['Target'])\n",
    "    correlations.append({'Feature': feat, 'Pearson_r': pearson_r, 'Abs_Pearson': abs(pearson_r)})\n",
    "corr_df = pd.DataFrame(correlations).sort_values('Abs_Pearson', ascending=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "ax1 = axes[0, 0]\n",
    "top_corr = corr_df.head(15)\n",
    "colors = ['#e74c3c' if x < 0 else '#2ecc71' for x in top_corr['Pearson_r']]\n",
    "ax1.barh(top_corr['Feature'], top_corr['Pearson_r'], color=colors, edgecolor='black')\n",
    "ax1.axvline(x=0, color='black', linewidth=0.5)\n",
    "ax1.axvline(x=0.1, color='blue', linestyle='--', alpha=0.5, label='|r|=0.1')\n",
    "ax1.axvline(x=-0.1, color='blue', linestyle='--', alpha=0.5)\n",
    "ax1.set_xlabel('Pearson Correlation'); ax1.set_title('Feature-Target Correlations', fontweight='bold')\n",
    "ax1.legend()\n",
    "\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(corr_df['Pearson_r'], bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "ax2.axvline(x=0, color='red', linewidth=2)\n",
    "ax2.axvline(x=corr_df['Pearson_r'].mean(), color='orange', linestyle='--', label=f'Mean: {corr_df[\"Pearson_r\"].mean():.4f}')\n",
    "ax2.set_xlabel('Pearson Correlation'); ax2.set_title('Correlation Distribution', fontweight='bold')\n",
    "ax2.legend()\n",
    "\n",
    "ax3 = axes[1, 0]\n",
    "very_weak = (corr_df['Abs_Pearson'] < 0.05).sum()\n",
    "weak = ((corr_df['Abs_Pearson'] >= 0.05) & (corr_df['Abs_Pearson'] < 0.1)).sum()\n",
    "moderate = ((corr_df['Abs_Pearson'] >= 0.1) & (corr_df['Abs_Pearson'] < 0.2)).sum()\n",
    "strong = (corr_df['Abs_Pearson'] >= 0.2).sum()\n",
    "categories = ['Very Weak\\n(<0.05)', 'Weak\\n(0.05-0.1)', 'Moderate\\n(0.1-0.2)', 'Strong\\n(â‰¥0.2)']\n",
    "counts = [very_weak, weak, moderate, strong]\n",
    "bars = ax3.bar(categories, counts, color=['#e74c3c', '#f39c12', '#3498db', '#2ecc71'], edgecolor='black')\n",
    "ax3.set_ylabel('Number of Features'); ax3.set_title('Signal Strength Distribution', fontweight='bold')\n",
    "for bar, count in zip(bars, counts):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3, f'{count}', ha='center')\n",
    "\n",
    "ax4 = axes[1, 1]\n",
    "ax4.axis('off')\n",
    "max_corr = corr_df['Abs_Pearson'].max()\n",
    "mean_corr = corr_df['Abs_Pearson'].mean()\n",
    "pct_weak = (corr_df['Abs_Pearson'] < 0.1).sum() / len(corr_df) * 100\n",
    "summary = f\"\"\"\n",
    "Signal-to-Noise Summary\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "Features: {len(corr_df)}\n",
    "Max |r|:  {max_corr:.4f}\n",
    "Mean |r|: {mean_corr:.4f}\n",
    "\n",
    "Very Weak: {very_weak} ({very_weak/len(corr_df)*100:.0f}%)\n",
    "Weak:      {weak} ({weak/len(corr_df)*100:.0f}%)\n",
    "Moderate:  {moderate} ({moderate/len(corr_df)*100:.0f}%)\n",
    "Strong:    {strong} ({strong/len(corr_df)*100:.0f}%)\n",
    "\n",
    "âš ï¸ {pct_weak:.0f}% features have |r| < 0.1\n",
    "\n",
    "Conclusion: LOW signal-to-noise\n",
    "ratio explains AUC â‰ˆ 0.50\n",
    "\"\"\"\n",
    "ax4.text(0.1, 0.9, summary, transform=ax4.transAxes, fontsize=13, verticalalignment='top',\n",
    "         fontfamily='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "plt.tight_layout()\n",
    "plt.savefig('signal_to_noise_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n  Max |correlation|: {max_corr:.4f}\")\n",
    "print(f\"  Features with |r| < 0.1: {pct_weak:.0f}%\")\n",
    "print(\"\\n Additional plots generated!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
